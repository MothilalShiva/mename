{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2589932",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import time\n",
    "from collections import Counter\n",
    "import json\n",
    "import boto3\n",
    "import zstandard as zstd\n",
    "from io import BytesIO\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def cluster_material_descriptions(input_file, material_type='FIN', n_clusters=3000, \n",
    "                                 similarity_threshold=0.75, min_cluster_size=5,\n",
    "                                 bucket=None, bucket_prefix='',\n",
    "                                 deployment_path='deployment_package'):\n",
    "    \"\"\"\n",
    "    Efficient clustering approach with minimal normalization and optimized merging.\n",
    "    Embeddings are stored in S3, models and metadata are stored locally.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    print(f\"Starting clustering for {material_type} with initial K={n_clusters}\")\n",
    "    \n",
    "    # Load and filter data\n",
    "    print(\"Loading data...\")\n",
    "    df = pd.read_csv(input_file)\n",
    "    df_fin = df[df['material_type'] == material_type].copy().reset_index(drop=True)\n",
    "    print(f\"Found {len(df_fin)} {material_type} records\")\n",
    "    \n",
    "    # Minimal text cleaning\n",
    "    def clean_text(text):\n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r'[^a-z0-9\\s\\/\\-\\.]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    \n",
    "    df_fin['clean_desc'] = df_fin['material_description'].apply(clean_text)\n",
    "    \n",
    "    # Generate embeddings\n",
    "    print(\"Generating embeddings...\")\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    batch_size = 2000\n",
    "    embeddings = []\n",
    "    \n",
    "    for i in range(0, len(df_fin), batch_size):\n",
    "        batch_descs = df_fin['clean_desc'].iloc[i:i+batch_size].tolist()\n",
    "        batch_embeddings = model.encode(batch_descs, show_progress_bar=False)\n",
    "        embeddings.extend(batch_embeddings)\n",
    "        if (i // batch_size) % 10 == 0:\n",
    "            print(f\"Processed {min(i+batch_size, len(df_fin))}/{len(df_fin)}\")\n",
    "    \n",
    "    embeddings = np.array(embeddings)\n",
    "    \n",
    "    # Store embeddings in S3 with zstandard compression\n",
    "    if bucket:\n",
    "        print(\"Storing embeddings in S3 with zstandard compression...\")\n",
    "        s3_client = boto3.client('s3')\n",
    "        \n",
    "        # Convert to float16 for size reduction\n",
    "        embeddings_16 = embeddings.astype(np.float16)\n",
    "        \n",
    "        # Serialize to bytes\n",
    "        buffer = BytesIO()\n",
    "        np.save(buffer, embeddings_16)\n",
    "        serialized_data = buffer.getvalue()\n",
    "        \n",
    "        # Compress with zstandard\n",
    "        compressor = zstd.ZstdCompressor(level=3)\n",
    "        compressed_data = compressor.compress(serialized_data)\n",
    "        \n",
    "        # Create S3 path for embeddings\n",
    "        prefix = f\"{bucket_prefix}/\" if bucket_prefix else \"\"\n",
    "        embeddings_s3_key = f\"{prefix}embeddings/{material_type}/embeddings.npy.zst\"\n",
    "        \n",
    "        # Store in S3\n",
    "        s3_client.put_object(\n",
    "            Bucket=bucket,\n",
    "            Key=embeddings_s3_key,\n",
    "            Body=compressed_data,\n",
    "            Metadata={\n",
    "                \"material_type\": material_type,\n",
    "                \"dimensions\": f\"{embeddings_16.shape}\",\n",
    "                \"dtype\": \"float16\",\n",
    "                \"compression\": \"zstandard\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Create metadata.json file (stored locally in deployment package)\n",
    "        metadata = {\n",
    "            \"material_type\": material_type,\n",
    "            \"embedding_path\": embeddings_s3_key,\n",
    "            \"embedding_bucket\": bucket,\n",
    "            \"model_version\": \"v1\",\n",
    "            \"training_date\": time.strftime(\"%Y-%m-%d\"),\n",
    "            \"num_samples\": len(embeddings)\n",
    "        }\n",
    "        \n",
    "        # Ensure the material type directory exists in deployment package\n",
    "        material_dir = os.path.join(deployment_path, material_type)\n",
    "        os.makedirs(material_dir, exist_ok=True)\n",
    "        \n",
    "        with open(os.path.join(material_dir, \"metadata.json\"), \"w\") as f:\n",
    "            json.dump(metadata, f, indent=4)\n",
    "        \n",
    "        print(f\"Embeddings stored at: s3://{bucket}/{embeddings_s3_key}\")\n",
    "    \n",
    "    # Perform initial clustering\n",
    "    print(f\"Clustering with K={n_clusters}...\")\n",
    "    kmeans = MiniBatchKMeans(n_clusters=n_clusters, random_state=42, \n",
    "                            batch_size=500, n_init=3, max_iter=50)\n",
    "    cluster_labels = kmeans.fit_predict(embeddings)\n",
    "    \n",
    "    # Calculate cluster sizes\n",
    "    cluster_sizes = pd.Series(cluster_labels).value_counts()\n",
    "    \n",
    "    # Optimized merging approach\n",
    "    print(\"Performing optimized merging...\")\n",
    "    \n",
    "    # Only merge very small clusters with their nearest neighbor\n",
    "    small_clusters = cluster_sizes[cluster_sizes < min_cluster_size].index.tolist()\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    \n",
    "    # Precompute similarities for small clusters\n",
    "    small_cluster_centroids = centroids[small_clusters]\n",
    "    all_centroids = centroids\n",
    "    \n",
    "    # Use matrix operations for faster similarity calculation\n",
    "    similarity_matrix = cosine_similarity(small_cluster_centroids, all_centroids)\n",
    "    \n",
    "    # For each small cluster, find the most similar non-small cluster\n",
    "    for idx, small_cluster in enumerate(small_clusters):\n",
    "        # Set self-similarity to -1 to avoid matching with itself\n",
    "        similarity_matrix[idx, small_cluster] = -1\n",
    "        \n",
    "        # Find the most similar cluster\n",
    "        most_similar = np.argmax(similarity_matrix[idx])\n",
    "        similarity_score = similarity_matrix[idx, most_similar]\n",
    "        \n",
    "        # Merge if similarity is above threshold\n",
    "        if similarity_score > similarity_threshold:\n",
    "            cluster_labels[cluster_labels == small_cluster] = most_similar\n",
    "    \n",
    "    # Renumber clusters\n",
    "    unique_clusters = np.unique(cluster_labels)\n",
    "    mapping = {old: new for new, old in enumerate(unique_clusters)}\n",
    "    final_labels = np.array([mapping[label] for label in cluster_labels])\n",
    "    \n",
    "    # Generate cluster names\n",
    "    print(\"Generating cluster names...\")\n",
    "    cluster_names = {}\n",
    "    \n",
    "    for cluster_id in range(len(unique_clusters)):\n",
    "        cluster_indices = np.where(final_labels == cluster_id)[0]\n",
    "        cluster_descs = df_fin['material_description'].iloc[cluster_indices].tolist()\n",
    "        \n",
    "        if not cluster_descs:\n",
    "            cluster_names[cluster_id] = f\"cluster_{cluster_id}\"\n",
    "            continue\n",
    "        \n",
    "        # Use the most common description as the cluster name\n",
    "        desc_counter = Counter(cluster_descs)\n",
    "        most_common_desc = desc_counter.most_common(1)[0][0]\n",
    "        \n",
    "        # Clean for use as a cluster name\n",
    "        clean_name = clean_text(most_common_desc)\n",
    "        clean_name = re.sub(r'\\s+', '_', clean_name)\n",
    "        clean_name = clean_name[:50]  # Limit length\n",
    "        \n",
    "        cluster_names[cluster_id] = clean_name\n",
    "    \n",
    "    # Add results to dataframe\n",
    "    df_fin['proposedkey'] = [cluster_names[label] for label in final_labels]\n",
    "    df_fin['cluster'] = final_labels\n",
    "    \n",
    "    # Sort the dataframe by cluster number\n",
    "    df_fin = df_fin.sort_values('cluster').reset_index(drop=True)\n",
    "    \n",
    "    # Create cluster_keywords.json file for manual editing (stored locally)\n",
    "    cluster_keywords = df_fin[['cluster', 'proposedkey']].drop_duplicates().set_index('cluster')['proposedkey'].to_dict()\n",
    "    \n",
    "    # Ensure the material type directory exists in deployment package\n",
    "    material_dir = os.path.join(deployment_path, material_type)\n",
    "    os.makedirs(material_dir, exist_ok=True)\n",
    "    \n",
    "    with open(os.path.join(material_dir, \"cluster_keywords.json\"), \"w\") as f:\n",
    "        json.dump(cluster_keywords, f, indent=4)\n",
    "    \n",
    "    # Save the model (stored locally in deployment package)\n",
    "    model_path = os.path.join(material_dir, \"model.pkl\")\n",
    "    with open(model_path, \"wb\") as f:\n",
    "        pickle.dump(kmeans, f)\n",
    "    \n",
    "    # Save output (sorted by cluster)\n",
    "    output_cols = ['material_number', 'material_type', 'material_description', 'proposedkey', 'cluster']\n",
    "    output_file = f'output_optimized_clustering_{material_type}.csv'\n",
    "    df_fin[output_cols].to_csv(output_file, index=False)\n",
    "    \n",
    "    # Print statistics\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    cluster_sizes = df_fin['cluster'].value_counts()\n",
    "    print(f\"\\nCluster Statistics:\")\n",
    "    print(f\"Total clusters: {len(cluster_sizes)}\")\n",
    "    print(f\"Clusters with > 10 items: {len(cluster_sizes[cluster_sizes > 10])}\")\n",
    "    print(f\"Clusters with > 5 items: {len(cluster_sizes[cluster_sizes > 5])}\")\n",
    "    print(f\"Clusters with 1 item: {len(cluster_sizes[cluster_sizes == 1])}\")\n",
    "    print(f\"Clusters with 2 items: {len(cluster_sizes[cluster_sizes == 2])}\")\n",
    "    print(f\"Processing time: {total_time/60:.2f} minutes\")\n",
    "    print(f\"Output saved to {output_file} (sorted by cluster number)\")\n",
    "    \n",
    "    # Print deployment information\n",
    "    print(f\"\\nDeployment Files Created:\")\n",
    "    print(f\"- Model: {model_path}\")\n",
    "    print(f\"- Cluster keywords: {os.path.join(material_dir, 'cluster_keywords.json')}\")\n",
    "    print(f\"- Metadata: {os.path.join(material_dir, 'metadata.json')}\")\n",
    "    if bucket:\n",
    "        print(f\"- Embeddings: s3://{bucket}/{embeddings_s3_key}\")\n",
    "    \n",
    "    return df_fin\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Parameters\n",
    "    input_file = 'input.csv'\n",
    "    material_type = 'FIN'\n",
    "    initial_k = 3000\n",
    "    similarity_threshold = 0.75\n",
    "    min_cluster_size = 5\n",
    "    \n",
    "    # S3 parameters\n",
    "    bucket = 'your-embedding-bucket'  # Replace with your S3 bucket name\n",
    "    bucket_prefix = 'keyword-recommendation'  # Optional prefix\n",
    "    \n",
    "    # Use your actual deployment package path\n",
    "    deployment_path = '/home/sagemaker-user/keyword-recommendation/sample_deployment/deployment_package'\n",
    "    \n",
    "    result = cluster_material_descriptions(\n",
    "        input_file, \n",
    "        material_type, \n",
    "        n_clusters=initial_k, \n",
    "        similarity_threshold=similarity_threshold,\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        bucket=bucket,\n",
    "        bucket_prefix=bucket_prefix,\n",
    "        deployment_path=deployment_path\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
