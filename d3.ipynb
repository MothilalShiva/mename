{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2a5118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import boto3\n",
    "import zstandard as zstd\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Set up logging to CloudWatch\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Global variables for model and embeddings\n",
    "sentence_model = None\n",
    "embeddings_cache = {}\n",
    "cluster_keywords_map = {}\n",
    "\n",
    "def download_embeddings_from_s3(bucket, key):\n",
    "    \"\"\"Download and decompress embeddings from S3\"\"\"\n",
    "    try:\n",
    "        s3_client = boto3.client('s3')\n",
    "        response = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "        compressed_data = response['Body'].read()\n",
    "        \n",
    "        # Decompress with zstandard\n",
    "        decompressor = zstd.ZstdDecompressor()\n",
    "        decompressed_data = decompressor.decompress(compressed_data)\n",
    "        \n",
    "        # Load numpy array\n",
    "        buffer = BytesIO(decompressed_data)\n",
    "        embeddings = np.load(buffer).astype(np.float32)\n",
    "        \n",
    "        return embeddings\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error downloading embeddings from S3: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"\n",
    "    Loads the KMeans model, embeddings from S3, and cluster-keyword mapping\n",
    "    from the model directory.\n",
    "    \"\"\"\n",
    "    global sentence_model, embeddings_cache, cluster_keywords_map\n",
    "    \n",
    "    logger.info(\"Starting model loading process\")\n",
    "    \n",
    "    # Load the metadata to get S3 path for embeddings\n",
    "    metadata_path = os.path.join(model_dir, 'metadata.json')\n",
    "    if not os.path.exists(metadata_path):\n",
    "        raise ValueError(f\"Metadata file not found at {metadata_path}\")\n",
    "    \n",
    "    with open(metadata_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    # Load the trained KMeans model\n",
    "    model_path = os.path.join(model_dir, 'model.pkl')\n",
    "    if not os.path.exists(model_path):\n",
    "        raise ValueError(f\"Model file not found at {model_path}\")\n",
    "    \n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    \n",
    "    logger.info(\"KMeans Model loaded\")\n",
    "    \n",
    "    # Load the cluster_keywords.json mapping\n",
    "    keywords_json_path = os.path.join(model_dir, 'cluster_keywords.json')\n",
    "    if not os.path.exists(keywords_json_path):\n",
    "        logger.warning(f\"{keywords_json_path} not found. 'proposedkey' will not be available via this mapping.\")\n",
    "        cluster_keywords_map = {}\n",
    "    else:\n",
    "        with open(keywords_json_path, 'r') as f:\n",
    "            # Convert keys to integers as cluster IDs are typically integers\n",
    "            raw_map = json.load(f)\n",
    "            cluster_keywords_map = {int(k): v for k, v in raw_map.items()}\n",
    "        logger.info(f\"Cluster keywords mapping loaded from {keywords_json_path}\")\n",
    "    \n",
    "    # Download embeddings from S3\n",
    "    bucket = metadata.get(\"embedding_bucket\")\n",
    "    embeddings_key = metadata.get(\"embedding_path\")\n",
    "    \n",
    "    if not bucket or not embeddings_key:\n",
    "        raise ValueError(\"Embedding bucket or path not found in metadata\")\n",
    "    \n",
    "    logger.info(f\"Downloading embeddings from s3://{bucket}/{embeddings_key}\")\n",
    "    embeddings = download_embeddings_from_s3(bucket, embeddings_key)\n",
    "    logger.info(\"Embeddings downloaded and decompressed\")\n",
    "    \n",
    "    # Load the sentence transformer model (same as used in training)\n",
    "    sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    logger.info(\"SentenceTransformer model loaded\")\n",
    "    \n",
    "    # Cache embeddings for faster prediction\n",
    "    embeddings_cache[metadata[\"material_type\"]] = embeddings\n",
    "    \n",
    "    # Return the loaded artifacts\n",
    "    return model, metadata, embeddings\n",
    "\n",
    "def input_fn(request_body, request_content_type):\n",
    "    \"\"\"\n",
    "    Parses the incoming request body. Expects 'application/json' content type.\n",
    "    \"\"\"\n",
    "    if request_content_type != 'application/json':\n",
    "        raise ValueError(f\"Unsupported content type: {request_content_type}. Only 'application/json' is supported.\")\n",
    "    \n",
    "    return json.loads(request_body)\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Minimal text cleaning - preserve industrial terminology as much as possible\n",
    "    Same as used during training\n",
    "    \"\"\"\n",
    "    text = str(text).lower()\n",
    "    # Keep alphanumeric, spaces, and common industrial symbols\n",
    "    text = re.sub(r'[^a-z0-9\\s\\/\\-\\.]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def predict_fn(input_data, model_and_metadata_and_embeddings):\n",
    "    \"\"\"\n",
    "    Makes predictions on the input data using the loaded model and embeddings.\n",
    "    Applies the same preprocessing steps as during training.\n",
    "    Retrieves the proposed keyword using the cluster ID and the loaded map.\n",
    "    \"\"\"\n",
    "    global sentence_model, embeddings_cache, cluster_keywords_map\n",
    "    \n",
    "    model, metadata, embeddings = model_and_metadata_and_embeddings\n",
    "    material_type = metadata.get(\"material_type\", \"unknown\")\n",
    "    \n",
    "    responses = []\n",
    "    \n",
    "    for data in input_data:\n",
    "        # Extract incoming data fields\n",
    "        incoming_component = data.get('component', '')\n",
    "        incoming_mattype = data.get('mattype', '')\n",
    "        incoming_componentdesc = data.get('componentdesc', '')\n",
    "        incoming_werks = data.get('werks', '')\n",
    "        incoming_aods = data.get('aods', '')\n",
    "        incoming_aoci = data.get('aoci', '')\n",
    "        incoming_keyword = data.get('keyword', '')\n",
    "        \n",
    "        predicted_cluster = None\n",
    "        output_proposedkey = None\n",
    "        \n",
    "        try:\n",
    "            # Clean the text (same as during training)\n",
    "            processed_desc = clean_text(incoming_componentdesc)\n",
    "            \n",
    "            if not processed_desc:\n",
    "                raise ValueError(\"Processed component description is empty or invalid after cleaning.\")\n",
    "            \n",
    "            # Generate embedding for the input description\n",
    "            input_embedding = sentence_model.encode([processed_desc])\n",
    "            \n",
    "            # Find the closest cluster by comparing with all embeddings\n",
    "            # Using cosine similarity (same as during training)\n",
    "            from sklearn.metrics.pairwise import cosine_similarity\n",
    "            similarities = cosine_similarity(input_embedding, embeddings)\n",
    "            predicted_cluster = np.argmax(similarities[0])\n",
    "            \n",
    "            logger.info(f\"Predicted cluster for '{processed_desc}': {predicted_cluster}\")\n",
    "            \n",
    "            # Get the proposed key from the pre-loaded map\n",
    "            if cluster_keywords_map:\n",
    "                output_proposedkey = cluster_keywords_map.get(predicted_cluster)\n",
    "                \n",
    "                if output_proposedkey:\n",
    "                    logger.info(f\"Found proposedkey for cluster {predicted_cluster}: {output_proposedkey}\")\n",
    "                else:\n",
    "                    logger.warning(f\"No 'proposedkey' found in map for cluster {predicted_cluster}. \" +\n",
    "                                  f\"Check cluster_keywords.json or if this cluster was in training data.\")\n",
    "            else:\n",
    "                logger.warning(\"Cluster keywords map not available. 'proposedkey' will be None.\")\n",
    "            \n",
    "            # Construct the response for the current item\n",
    "            response = {\n",
    "                'component': incoming_component,\n",
    "                'mattype': incoming_mattype,\n",
    "                'werks': incoming_werks,\n",
    "                'aoci': incoming_aoci,\n",
    "                'aods': incoming_aods,\n",
    "                'componentdesc': incoming_componentdesc,  # Use original incoming for output\n",
    "                'cluster': int(predicted_cluster),\n",
    "                'keyword': incoming_keyword,\n",
    "                'proposedkey': output_proposedkey\n",
    "            }\n",
    "            \n",
    "            responses.append(response)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing input data: {e}\", exc_info=True)\n",
    "            \n",
    "            # Append an error response for the failed item\n",
    "            error_response = {\n",
    "                'component': incoming_component,\n",
    "                'mattype': incoming_mattype,\n",
    "                'werks': incoming_werks,\n",
    "                'aoci': incoming_aoci,\n",
    "                'aods': incoming_aods,\n",
    "                'componentdesc': incoming_componentdesc,\n",
    "                'keyword': incoming_keyword,\n",
    "                'cluster': None,\n",
    "                'proposedkey': None,\n",
    "                'error': str(e)\n",
    "            }\n",
    "            \n",
    "            responses.append(error_response)\n",
    "    \n",
    "    return responses\n",
    "\n",
    "def output_fn(prediction, content_type='application/json'):\n",
    "    \"\"\"\n",
    "    Serializes the prediction result to the specified content type.\n",
    "    \"\"\"\n",
    "    return json.dumps(prediction), content_type"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
